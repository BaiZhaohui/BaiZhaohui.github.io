<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Keras functional APIKeras functional API 是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。例1：全连接网络其实对于实现全连接网络，Sequential模型是更好的选择。  层的实例可以被调用（on a tensor)，并且返回一个tensor 使用输入输出张量来定义Model 这种模型可以像Sequential模型一样训练。  1">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/2019/08/14/2019-4-15-Keras functional API/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Keras functional APIKeras functional API 是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。例1：全连接网络其实对于实现全连接网络，Sequential模型是更好的选择。  层的实例可以被调用（on a tensor)，并且返回一个tensor 使用输入输出张量来定义Model 这种模型可以像Sequential模型一样训练。  1">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-08-11T11:52:17.299Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
<meta name="twitter:description" content="Keras functional APIKeras functional API 是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。例1：全连接网络其实对于实现全连接网络，Sequential模型是更好的选择。  层的实例可以被调用（on a tensor)，并且返回一个tensor 使用输入输出张量来定义Model 这种模型可以像Sequential模型一样训练。  1">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-2019-4-15-Keras functional API" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/14/2019-4-15-Keras functional API/" class="article-date">
  <time datetime="2019-08-14T15:21:03.240Z" itemprop="datePublished">2019-08-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Keras-functional-API"><a href="#Keras-functional-API" class="headerlink" title="Keras functional API"></a>Keras functional API</h3><h5 id="Keras-functional-API-是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。"><a href="#Keras-functional-API-是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。" class="headerlink" title="Keras functional API 是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。"></a>Keras functional API 是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。</h5><h4 id="例1：全连接网络"><a href="#例1：全连接网络" class="headerlink" title="例1：全连接网络"></a>例1：全连接网络</h4><p>其实对于实现全连接网络，<code>Sequential</code>模型是更好的选择。</p>
<ul>
<li>层的实例可以被调用（on a tensor)，并且返回一个tensor</li>
<li>使用输入输出张量来定义<code>Model</code></li>
<li>这种模型可以像<code>Sequential</code>模型一样训练。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Input,Dense</span><br><span class="line">from keras.models import Model</span><br><span class="line"># This returns a tensor</span><br><span class="line">inputs = Input(shape=(784,))</span><br><span class="line"></span><br><span class="line"># a layer instance is callable on a tensor, and returns a tensor</span><br><span class="line">x = Dense(64,activation=&apos;relu&apos;)(inputs)</span><br><span class="line">x = Dense(64,activation=&apos;relu&apos;)(x)</span><br><span class="line">predictions = Dense(10,activation=&apos;softmax&apos;)(x)</span><br><span class="line"></span><br><span class="line"># This creates a model that includes</span><br><span class="line"># the Input layer and three Dense layers</span><br><span class="line">model = Model(inputs=inputs,outputs=predictions)</span><br><span class="line">model.compile(optimizer=&apos;rmsprop&apos;,</span><br><span class="line">              loss=&apos;categorical_crossentropy&apos;,</span><br><span class="line">              metrics=[&apos;accuracy&apos;])</span><br><span class="line">model.fit(data,labels) # starts training</span><br></pre></td></tr></table></figure>

<h4 id="所有模型都可以像层一样调用"><a href="#所有模型都可以像层一样调用" class="headerlink" title="所有模型都可以像层一样调用"></a>所有模型都可以像层一样调用</h4><h5 id="使用功能API，可以轻松地重用经过训练的模型：您可以通过在张量上调用任何模型来将其视为一个层。请注意，通过调用模型，您不仅可以重用模型的体系结构，还可以重用其权重。"><a href="#使用功能API，可以轻松地重用经过训练的模型：您可以通过在张量上调用任何模型来将其视为一个层。请注意，通过调用模型，您不仅可以重用模型的体系结构，还可以重用其权重。" class="headerlink" title="使用功能API，可以轻松地重用经过训练的模型：您可以通过在张量上调用任何模型来将其视为一个层。请注意，通过调用模型，您不仅可以重用模型的体系结构，还可以重用其权重。"></a>使用功能API，可以轻松地重用经过训练的模型：您可以通过在张量上调用任何模型来将其视为一个层。请注意，通过调用模型，您不仅可以重用模型的体系结构，还可以重用其权重。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Input(shape=(784,))</span><br><span class="line"># This works, and returns the 10-way softmax we defined above.</span><br><span class="line">y = model(x)</span><br></pre></td></tr></table></figure>

<h5 id="这可以允许快速创建可以处理输入序列的模型。您可以将图像分类模型转换为视频分类模型，只需一行。"><a href="#这可以允许快速创建可以处理输入序列的模型。您可以将图像分类模型转换为视频分类模型，只需一行。" class="headerlink" title="这可以允许快速创建可以处理输入序列的模型。您可以将图像分类模型转换为视频分类模型，只需一行。"></a>这可以允许快速创建可以处理输入序列的模型。您可以将图像分类模型转换为视频分类模型，只需一行。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import TimeDistributed</span><br><span class="line"># Input tensor for sequences of 20 timesteps</span><br><span class="line"># each containing a 784-dimensional vector</span><br><span class="line">input_sequences =Input(shape=(20,784))</span><br><span class="line"></span><br><span class="line"># This applies our previous model to every timestep in the input sequences.</span><br><span class="line"># the output of the previous model was a 10-way softmax</span><br><span class="line"># so the output of the layer below will be a sequence of 20 vectors of size 10.</span><br><span class="line">processed_sequences = TimeDistributed(model)(input_sequences)</span><br></pre></td></tr></table></figure>

<h4 id="多输入和多输出模型"><a href="#多输入和多输出模型" class="headerlink" title="多输入和多输出模型"></a>多输入和多输出模型</h4><h5 id="functional-API-使操作大量交织在一起的数据流变得容易。"><a href="#functional-API-使操作大量交织在一起的数据流变得容易。" class="headerlink" title="functional API 使操作大量交织在一起的数据流变得容易。"></a>functional API 使操作大量交织在一起的数据流变得容易。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from keras.layres import Input,Embedding,LSTM,Dense</span><br><span class="line">from keras.models import Model</span><br><span class="line"></span><br><span class="line"># Headline input: meant to receive sequences of 100 integersm,between 1 and 10000.</span><br><span class="line"># Note that we can name any layer by passing it a &quot;name&quot; argument.</span><br><span class="line">main_input = Input(shape=(100,),dtype=&apos;int32&apos;,name=&apos;main_input&apos;)</span><br><span class="line"></span><br><span class="line"># This embedding layer will encode the input sequence</span><br><span class="line"># into a sequence of dense 512-dimensional vectors.</span><br><span class="line">x = Embedding(output_dim=512,input_dim=10000,input_length=100)(main_input)</span><br><span class="line"></span><br><span class="line"># A LSTM will transform the vector sequence into a single vector,</span><br><span class="line"># containing information about the entire sequence</span><br><span class="line">lstm_out = LSTM(32)(x)</span><br><span class="line">auxiliary_output = Dense(1,activation=&apos;sigmoid&apos;,name=&apos;aux_output&apos;)(lstm_out)</span><br><span class="line"></span><br><span class="line">auxiliary_input = Input(shape=(5,),name=&apos;aux_input&apos;)</span><br><span class="line">x = keras.layers.concatenate([lstm_out,auxiliary_input])</span><br><span class="line"></span><br><span class="line"># We stack a deep densely-connected network on top</span><br><span class="line">x = Dense(64,activation=&apos;relu&apos;)(x)</span><br><span class="line">x = Dense(64,activation=&apos;relu&apos;)(x)</span><br><span class="line">x = Dense(64,activation=&apos;relu&apos;)(x)</span><br><span class="line"></span><br><span class="line"># And finally we add the main logistic regression layer</span><br><span class="line">main_output = Dense(1,activation=&apos;sigmoid&apos;,name=&apos;main_output&apos;)(x)</span><br></pre></td></tr></table></figure>

<h5 id="这定义了一个具有两个输入和两个输出的模型："><a href="#这定义了一个具有两个输入和两个输出的模型：" class="headerlink" title="这定义了一个具有两个输入和两个输出的模型："></a>这定义了一个具有两个输入和两个输出的模型：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = Model(inputs=[main_input,auxiliary_input],outputs=[main_output,auxiliary_output])</span><br></pre></td></tr></table></figure>

<h5 id="我们编译模型并为辅助损失分配0-2的权重。要为不同的输出指定不同的loss-weights或loss，可以使用列表或字典。这里我们传递一个损失作为loss参数，因此所有输出都将使用相同的损失。"><a href="#我们编译模型并为辅助损失分配0-2的权重。要为不同的输出指定不同的loss-weights或loss，可以使用列表或字典。这里我们传递一个损失作为loss参数，因此所有输出都将使用相同的损失。" class="headerlink" title="我们编译模型并为辅助损失分配0.2的权重。要为不同的输出指定不同的loss_weights或loss，可以使用列表或字典。这里我们传递一个损失作为loss参数，因此所有输出都将使用相同的损失。"></a>我们编译模型并为辅助损失分配0.2的权重。要为不同的输出指定不同的loss_weights或loss，可以使用列表或字典。这里我们传递一个损失作为<code>loss参数</code>，因此所有输出都将使用相同的损失。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=&apos;rmsprop&apos;,loss=&apos;binary_crossentropy&apos;,</span><br><span class="line">              loss_weights=[1.,0.2])</span><br></pre></td></tr></table></figure>

<h5 id="我们可以通过传递输入数组和目标数组的列表来训练模型："><a href="#我们可以通过传递输入数组和目标数组的列表来训练模型：" class="headerlink" title="我们可以通过传递输入数组和目标数组的列表来训练模型："></a>我们可以通过传递输入数组和目标数组的列表来训练模型：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.fit([heading_data,additional_data],[labels,labels],</span><br><span class="line">          epochs=50,batch_size=32)</span><br></pre></td></tr></table></figure>

<h5 id="由于我们的输入和输出被命名（我们传递了一个“name”参数），我们也可以通过以下方式编译模型："><a href="#由于我们的输入和输出被命名（我们传递了一个“name”参数），我们也可以通过以下方式编译模型：" class="headerlink" title="由于我们的输入和输出被命名（我们传递了一个“name”参数），我们也可以通过以下方式编译模型："></a>由于我们的输入和输出被命名（我们传递了一个“<code>name</code>”参数），我们也可以通过以下方式编译模型：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=&apos;rmsprop&apos;,</span><br><span class="line">              loss=&#123;&apos;main_output&apos;:&apos;binary_crossentropy&apos;,&apos;aux_output&apos;:&apos;binary_crossentropy&apos;&#125;,</span><br><span class="line">              loss_weight=&#123;&apos;main_output&apos;:1.,&apos;aux_output&apos;:0.2&#125;)</span><br><span class="line">              </span><br><span class="line"># And trained it via:</span><br><span class="line">model.fit(&#123;&apos;main_input&apos;:headline_data,&apos;aux_output&apos;:additional_data&#125;,</span><br><span class="line">          &#123;&apos;main_output&apos;:labels,&apos;aux_ouput&apos;:labels&#125;,</span><br><span class="line">          epochs=50,batch_size=32)</span><br></pre></td></tr></table></figure>

<h5 id="共享层"><a href="#共享层" class="headerlink" title="共享层"></a>共享层</h5><p>使用函数API的另一个好处是模型可以使用共享层。<br>例子：<br>建立一个模型来判断两条推特是不是同一个用户所发。（可以通过比较推文的相似性来确定）<br>将两条推文编码成两个向量并连接，添加逻辑回归层，这将输出两条推文来自同一用户的概率。使用来自同一用户的两条推文（正），不是同一用户所发的两条推文（负）来训练模型。<br>由于问题是对称的，编码第一条推文的机制（包括权重等）将被重用来编码第二条推文。这里使用LSTM层来编码推文。<br>使用函数式 API 来构建模型。首先我们将一条推特转换为一个尺寸为 (280, 256) 的矩阵，即每条推特 280 字符，每个字符为 256 维的 one-hot 编码向量 （取 256 个常用字符）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import keras</span><br><span class="line">from keras.layers import Input,LSTM,Dense</span><br><span class="line">from keras.models import Model</span><br><span class="line"></span><br><span class="line">tweet_a = Input(shape=(280,256)</span><br><span class="line">tweet_b = Input(shape=(280,256)</span><br></pre></td></tr></table></figure>

<p>要在不同的输入上共享同一个层，只需实例化该层一次，然后根据需要传入你想要的输入即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># This layer can take as input a matrix</span><br><span class="line"># and will return a vector of size 64</span><br><span class="line">shared_lstm = LSTM(64)</span><br><span class="line"></span><br><span class="line"># When we reuse the same layer instance</span><br><span class="line"># multiple times,the weights of layer</span><br><span class="line"># are also being reused</span><br><span class="line"># (it is effectively *the same* layer)</span><br><span class="line">encoded_a = shared_lstm(tweet_a)</span><br><span class="line">encoded_b = shared_lstm(tweet_b)</span><br><span class="line"></span><br><span class="line"># We can then concatenate the two vectors:</span><br><span class="line">merged_vector = keras.layers.concatenate([encoded_a,encoded_b]),axis=-1)</span><br><span class="line"></span><br><span class="line"># And add a logistic regression on top</span><br><span class="line">predictions = Dense(1,activation=&apos;sigmoid&apos;)(merged_vector)</span><br><span class="line"></span><br><span class="line"># We defined a trainable model linking the</span><br><span class="line"># tweet inputs to the predictions</span><br><span class="line"># 定义一个连接推特输入和预测的可训练的模型</span><br><span class="line">model = Model(inputs=[tweet_a,tweet_b],outputs=predictions)</span><br><span class="line">model.compile(optimizer=&apos;rmsprop&apos;,</span><br><span class="line">              loss=&apos;binary_crossentropy&apos;,</span><br><span class="line">              metrics=[&apos;accuracy&apos;])</span><br><span class="line">model.fit([data_a,data_b],labels,epochs=10)</span><br></pre></td></tr></table></figure>

<h5 id="如何读取共享层的输出或输出尺寸"><a href="#如何读取共享层的输出或输出尺寸" class="headerlink" title="如何读取共享层的输出或输出尺寸?"></a>如何读取共享层的输出或输出尺寸?</h5><h4 id="层节点（The-concept-of-layer-“node”）"><a href="#层节点（The-concept-of-layer-“node”）" class="headerlink" title="层节点（The concept of layer “node”）"></a>层节点（The concept of layer “node”）</h4><p>每当你在某个输入上调用一个层时，都将创建一个新的张量（层的输出），并且为该层添加一个「节点」，将输入张量连接到输出张量。当多次调用同一个图层时，该图层将拥有多个节点索引 (0, 1, 2…)。<br>在之前版本的 Keras 中，可以通过 <code>layer.get_output()</code> 来获得层实例的输出张量，或者通过 <code>layer.output_shape</code> 来获取其输出形状。现在你依然可以这么做（除了 <code>get_output()</code> 已经被 <code>output</code> 属性替代）。但是如果一个层与多个输入连接呢？</p>
<p>只要一个层仅仅连接到一个输入，就不会有困惑，<code>.output</code> 会返回层的唯一输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = Input(shape=(280,256))</span><br><span class="line">lstm = LSTM(32)</span><br><span class="line">encoded_a = lstm(a)</span><br><span class="line">assert lstm.output == encoded_a</span><br></pre></td></tr></table></figure>

<p>但是如果该层有多个输入，那就会出现问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = Input(shape=(280,256))</span><br><span class="line">b = Input(shape=(280,256))</span><br><span class="line"></span><br><span class="line">lstm = LSTM(32)</span><br><span class="line">encoded_a = lstm(a)</span><br><span class="line">encoded_b = lstm(b)</span><br><span class="line"></span><br><span class="line">lstm.output</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; AttributeError: Layer lstm_1 has multiple inbound nodes,</span><br><span class="line">hence the notion of &quot;layer output&quot; is ill-defined.</span><br><span class="line">Use `get_output_at(node_index)` instead.</span><br></pre></td></tr></table></figure>

<p>解决方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">assert lstm.get_output_at(0) == encoded_a</span><br><span class="line">assert lstm.get_output_at(1) == encoded_b</span><br></pre></td></tr></table></figure>

<p><code>input_shape</code> 和 <code>output_shape</code> 这两个属性也是如此：只要该层只有一个节点，或者只要所有节点具有相同的输入/输出尺寸，那么「层输出/输入尺寸」的概念就被很好地定义，并且将由 <code>layer.output_shape / layer.input_shape</code> 返回。但是比如说，如果将一个 <code>Conv2D</code> 层先应用于尺寸为 <code>(32，32，3)</code> 的输入，再应用于尺寸为 <code>(64, 64, 3)</code> 的输入，那么这个层就会有多个输入/输出尺寸，你将不得不通过指定它们所属节点的索引来获取它们：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = Input(shape=(32,32,3))</span><br><span class="line">b = Input(shape=(64,64,3))</span><br><span class="line"></span><br><span class="line">conv = Conv2D(16,(3,3),padding=&apos;same&apos;)</span><br><span class="line">conved_a = conv(a)</span><br><span class="line"></span><br><span class="line">#Only one input so far,the following will work:</span><br><span class="line">assert conv.input_shape == (None,32,32,3)</span><br><span class="line"></span><br><span class="line">conved_b = conv(b)</span><br><span class="line"># now the &apos;.input_shape&apos; property wouldn&apos;t work,but this does:</span><br><span class="line">assert conv.get_input_shape_at(0) == (None,32,32,3)</span><br><span class="line">assert conv.get_input_shape_at(1) == (None,64,64,3)</span><br></pre></td></tr></table></figure>

<h4 id="更多的例子"><a href="#更多的例子" class="headerlink" title="更多的例子"></a>更多的例子</h4><h5 id="Inception模型"><a href="#Inception模型" class="headerlink" title="Inception模型"></a>Inception模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Conv2D,MaxPooling2D,Input</span><br><span class="line"></span><br><span class="line">input_img = Input(shape=(256,256,3))</span><br><span class="line"></span><br><span class="line">tower_1 = Conv2D(64,(1,1),padding=&apos;same&apos;,activation=&apos;relu&apos;)(input_img)</span><br><span class="line">tower_1 = Conv2D(64,(3,3),padding=&apos;same&apos;,activation=&apos;relu&apos;)(tower_1)</span><br><span class="line"></span><br><span class="line">tower_2 = Conv2D(64,(1,1),padding=&apos;same&apos;,activation=&apos;relu&apos;)(input_img)</span><br><span class="line">tower_2 = Conv2D(64,(5,5),padding=&apos;same&apos;,activation=&apos;relu&apos;)(tower_2)</span><br><span class="line"></span><br><span class="line">tower_3 = MaxPooling2D((3,3),strides=(1,1),padding=&apos;same&apos;)(input_img)</span><br><span class="line">tower_3 = Conv2D(64,(1,1),padding=&apos;same&apos;,activation=&apos;relu&apos;)(tower_3)</span><br><span class="line"></span><br><span class="line">output = keras.layers.concatenate([tower_1,tower_2,tower_3],axis=1)</span><br></pre></td></tr></table></figure>

<h5 id="卷积层上的残差连接"><a href="#卷积层上的残差连接" class="headerlink" title="卷积层上的残差连接"></a>卷积层上的残差连接</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Conv2D,Input</span><br><span class="line"># input tensor for a 3-channel 256x256 image</span><br><span class="line">x = Input(shape=(256,256,3))</span><br><span class="line"># 3x3 conv with 3 output channels (same as input channels)</span><br><span class="line">y = Conv2D(3,(3,3),padding=&apos;same&apos;)(x)</span><br><span class="line"># this returns x+y</span><br><span class="line">z = keras.layers.add([x,y])</span><br></pre></td></tr></table></figure>

<h5 id="共享视觉模型"><a href="#共享视觉模型" class="headerlink" title="共享视觉模型"></a>共享视觉模型</h5><p>该模型在两个输入上重复使用同一个图像处理模块，以判断两个 MNIST 数字是否为相同的数字。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Conv2D,MaxPooling2D,Input,Dense,Flatten</span><br><span class="line">from keras.models import Model</span><br><span class="line"></span><br><span class="line"># First,define the vision modules</span><br><span class="line">digit_input = Input(shape=(27,27,1))</span><br><span class="line">x = Conv2D(64,(3,3))(digit_input)</span><br><span class="line">x = Conv2D(64,(3,3))(x)</span><br><span class="line">x = MaxPooling2D((2,2))(x)</span><br><span class="line">out = Flatten()(x)</span><br><span class="line"></span><br><span class="line">vision_model = Model(digit_input,out)</span><br><span class="line"></span><br><span class="line"># Then define the tell-digits-apart model</span><br><span class="line">digit_a = Input(shape=(27,27,1))</span><br><span class="line">digit_b = Input(shape=(27,27,1))</span><br><span class="line"></span><br><span class="line"># The vision model will be shared,weights and all</span><br><span class="line">out_a = vision_model(digit_a)</span><br><span class="line">out_b = vision_model(digit_b)</span><br><span class="line"></span><br><span class="line">concatenated = keras.layers.concatenate([out_a,out_b])</span><br><span class="line">out = Dense(1,activation=&apos;sigmoid&apos;)(concatenated)</span><br><span class="line"></span><br><span class="line">classification_model = Model([digit_a,digit_b],out)</span><br></pre></td></tr></table></figure>

<h5 id="视觉问答模型"><a href="#视觉问答模型" class="headerlink" title="视觉问答模型"></a>视觉问答模型</h5><p>当被问及关于图片的自然语言问题时，该模型可以选择正确的单词作答。<br>它通过将问题和图像编码成向量，然后连接两者，在上面训练一个逻辑回归，来从词汇表中挑选一个可能的单词作答。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Conv2D,MaxPooling2D,Flatten</span><br><span class="line">from keras.layers import Input,LSTM,Embedding,Dense</span><br><span class="line">from keras.models import Model,Sequential</span><br><span class="line"># First,let&apos;s define a vision model using a Sequential model</span><br><span class="line"># This model will encode an image into a vector</span><br><span class="line">vision_model = Sequential()</span><br><span class="line">vision_model.add(Conv2D(64,(3,3),activation=&apos;relu&apos;,padding=&apos;same&apos;,input_shape=(224,224,3)))</span><br><span class="line">vision_model.add(Conv2D(64,(3,3),activation=&apos;relu&apos;))</span><br><span class="line">vision_model.add(MaxPooling2D((2,2)))</span><br><span class="line">vision_model.add(Conv2D(128,(3,3),activation=&apos;relu&apos;,padding=&apos;same&apos;))</span><br><span class="line">vision_model.add(Conv2D(128,(3,3),activation=&apos;relu&apos;))</span><br><span class="line">vision_model.add(MaxPooling2D((2,2)))</span><br><span class="line">vision_model.add(Conv2D(256,(3,3),activation=&apos;relu&apos;,padding=&apos;same&apos;))</span><br><span class="line">vision_model.add(Conv2D(256,(3,3),activation=&apos;relu&apos;))</span><br><span class="line">vision_model.add(Conv2D(256,(3,3),activation=&apos;relu&apos;))</span><br><span class="line">vision_model.add(MaxPooling2D((2,2)))</span><br><span class="line">vision_model.add(Flatten())</span><br><span class="line"></span><br><span class="line"># Now let&apos;s get a tensor with the output of our vision model:</span><br><span class="line">image_input = Input(shape=(224,224,3))</span><br><span class="line">encoded_image = vision_model(image_input)</span><br><span class="line"></span><br><span class="line"># Next,let&apos;s define a language model to encode the question into a vector.</span><br><span class="line"># Each question will be at most 100 word long,</span><br><span class="line"># and we will index words as integers from 1 to 9999</span><br><span class="line">question_input = Input(shape=(100,),dtype=&apos;int32&apos;)</span><br><span class="line">embedded_question = Embedding(input_dim=10000,output_dim=256,input_length=100)(question_input)</span><br><span class="line">encoded_question = LSTM(256)(embedded_question)</span><br><span class="line"></span><br><span class="line"># Let&apos;s concatenate the question vector and the image vector:</span><br><span class="line">merged = keras.layers.concatenate([encoded_question,encoded_image])</span><br><span class="line"></span><br><span class="line"># And let&apos;s train a logistic regression over 1000 words on top:</span><br><span class="line">output = Dense(1000,activation=&apos;softmax&apos;)(merged)</span><br><span class="line"></span><br><span class="line"># This is our final model:</span><br><span class="line">vqa_model = Model(inputs=[image_input,question_input],outputs=output)</span><br><span class="line"># The next stage would be training this model on actual data.</span><br></pre></td></tr></table></figure>

<h5 id="视频问答模型"><a href="#视频问答模型" class="headerlink" title="视频问答模型"></a>视频问答模型</h5><p>现在我们已经训练了图像问答模型，我们可以很快地将它转换为视频问答模型。在适当的训练下，你可以给它展示一小段视频（例如 100 帧的人体动作），然后问它一个关于这段视频的问题（例如，「这个人在做什么运动？」 -&gt; 「足球」）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import TimeDistributed</span><br><span class="line"></span><br><span class="line">video_input = Input(shape=(100, 224, 224, 3))</span><br><span class="line"># 这是基于之前定义的视觉模型（权重被重用）构建的视频编码</span><br><span class="line">encoded_frame_sequence = TimeDistributed(vision_model)(video_input)  # 输出为向量的序列</span><br><span class="line">encoded_video = LSTM(256)(encoded_frame_sequence)  # 输出为一个向量</span><br><span class="line"></span><br><span class="line"># 这是问题编码器的模型级表示，重复使用与之前相同的权重：</span><br><span class="line">question_encoder = Model(inputs=question_input, outputs=encoded_question)</span><br><span class="line"></span><br><span class="line"># 让我们用它来编码这个问题：</span><br><span class="line">video_question_input = Input(shape=(100,), dtype=&apos;int32&apos;)</span><br><span class="line">encoded_video_question = question_encoder(video_question_input)</span><br><span class="line"></span><br><span class="line"># 这就是我们的视频问答模式：</span><br><span class="line">merged = keras.layers.concatenate([encoded_video, encoded_video_question])</span><br><span class="line">output = Dense(1000, activation=&apos;softmax&apos;)(merged)</span><br><span class="line">video_qa_model = Model(inputs=[video_input, video_question_input], outputs=output)</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/14/2019-4-15-Keras functional API/" data-id="cjzbeqqnx000cvjw8w7kt2pu9" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/08/14/2019-4-24-小球回弹/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2019/08/14/2018-4-8-meshgrid/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">np.meshgrid</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/08/14/2019-7-19-Python Objects Types and Expressions/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/14/2019-7-23-笔记整理（MOOC)/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/14/2019-7-23-笔记整理（侯捷笔记4)/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/14/2019-7-23-笔记整理（C++ADT)/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/14/2019-7-23-笔记整理（DSIPy)/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>