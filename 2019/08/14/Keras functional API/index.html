<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="Keras functional APIKeras functional API 是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。例1：全连接网络其实对于实现全连接网络，Sequential模型是更好的选择。  层的实例可以被调用（on a tensor)，并且返回一个tensor 使用输入输出张量来定义Model 这种模型可以像Sequential模型一样训练。  1">
<meta property="og:type" content="article">
<meta property="og:title" content="Mist">
<meta property="og:url" content="http://yoursite.com/2019/08/14/Keras functional API/index.html">
<meta property="og:site_name" content="Mist">
<meta property="og:description" content="Keras functional APIKeras functional API 是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。例1：全连接网络其实对于实现全连接网络，Sequential模型是更好的选择。  层的实例可以被调用（on a tensor)，并且返回一个tensor 使用输入输出张量来定义Model 这种模型可以像Sequential模型一样训练。  1">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-08-11T11:52:17.299Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mist">
<meta name="twitter:description" content="Keras functional APIKeras functional API 是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。例1：全连接网络其实对于实现全连接网络，Sequential模型是更好的选择。  层的实例可以被调用（on a tensor)，并且返回一个tensor 使用输入输出张量来定义Model 这种模型可以像Sequential模型一样训练。  1">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/08/14/Keras functional API/">





  <title> | Mist</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Mist</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div>
    
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">------ 本文结束------</div>
    
</div>
    
 </div>
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/14/Keras functional API/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhaohui Bai">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mist">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline"></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-14T23:21:03+08:00">
                2019-08-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="Keras-functional-API"><a href="#Keras-functional-API" class="headerlink" title="Keras functional API"></a>Keras functional API</h3><h5 id="Keras-functional-API-是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。"><a href="#Keras-functional-API-是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。" class="headerlink" title="Keras functional API 是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。"></a>Keras functional API 是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。</h5><h4 id="例1：全连接网络"><a href="#例1：全连接网络" class="headerlink" title="例1：全连接网络"></a>例1：全连接网络</h4><p>其实对于实现全连接网络，<code>Sequential</code>模型是更好的选择。</p>
<ul>
<li>层的实例可以被调用（on a tensor)，并且返回一个tensor</li>
<li>使用输入输出张量来定义<code>Model</code></li>
<li>这种模型可以像<code>Sequential</code>模型一样训练。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Input,Dense</span><br><span class="line">from keras.models import Model</span><br><span class="line"># This returns a tensor</span><br><span class="line">inputs = Input(shape=(784,))</span><br><span class="line"></span><br><span class="line"># a layer instance is callable on a tensor, and returns a tensor</span><br><span class="line">x = Dense(64,activation=&apos;relu&apos;)(inputs)</span><br><span class="line">x = Dense(64,activation=&apos;relu&apos;)(x)</span><br><span class="line">predictions = Dense(10,activation=&apos;softmax&apos;)(x)</span><br><span class="line"></span><br><span class="line"># This creates a model that includes</span><br><span class="line"># the Input layer and three Dense layers</span><br><span class="line">model = Model(inputs=inputs,outputs=predictions)</span><br><span class="line">model.compile(optimizer=&apos;rmsprop&apos;,</span><br><span class="line">              loss=&apos;categorical_crossentropy&apos;,</span><br><span class="line">              metrics=[&apos;accuracy&apos;])</span><br><span class="line">model.fit(data,labels) # starts training</span><br></pre></td></tr></table></figure>

<h4 id="所有模型都可以像层一样调用"><a href="#所有模型都可以像层一样调用" class="headerlink" title="所有模型都可以像层一样调用"></a>所有模型都可以像层一样调用</h4><h5 id="使用功能API，可以轻松地重用经过训练的模型：您可以通过在张量上调用任何模型来将其视为一个层。请注意，通过调用模型，您不仅可以重用模型的体系结构，还可以重用其权重。"><a href="#使用功能API，可以轻松地重用经过训练的模型：您可以通过在张量上调用任何模型来将其视为一个层。请注意，通过调用模型，您不仅可以重用模型的体系结构，还可以重用其权重。" class="headerlink" title="使用功能API，可以轻松地重用经过训练的模型：您可以通过在张量上调用任何模型来将其视为一个层。请注意，通过调用模型，您不仅可以重用模型的体系结构，还可以重用其权重。"></a>使用功能API，可以轻松地重用经过训练的模型：您可以通过在张量上调用任何模型来将其视为一个层。请注意，通过调用模型，您不仅可以重用模型的体系结构，还可以重用其权重。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Input(shape=(784,))</span><br><span class="line"># This works, and returns the 10-way softmax we defined above.</span><br><span class="line">y = model(x)</span><br></pre></td></tr></table></figure>

<h5 id="这可以允许快速创建可以处理输入序列的模型。您可以将图像分类模型转换为视频分类模型，只需一行。"><a href="#这可以允许快速创建可以处理输入序列的模型。您可以将图像分类模型转换为视频分类模型，只需一行。" class="headerlink" title="这可以允许快速创建可以处理输入序列的模型。您可以将图像分类模型转换为视频分类模型，只需一行。"></a>这可以允许快速创建可以处理输入序列的模型。您可以将图像分类模型转换为视频分类模型，只需一行。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import TimeDistributed</span><br><span class="line"># Input tensor for sequences of 20 timesteps</span><br><span class="line"># each containing a 784-dimensional vector</span><br><span class="line">input_sequences =Input(shape=(20,784))</span><br><span class="line"></span><br><span class="line"># This applies our previous model to every timestep in the input sequences.</span><br><span class="line"># the output of the previous model was a 10-way softmax</span><br><span class="line"># so the output of the layer below will be a sequence of 20 vectors of size 10.</span><br><span class="line">processed_sequences = TimeDistributed(model)(input_sequences)</span><br></pre></td></tr></table></figure>

<h4 id="多输入和多输出模型"><a href="#多输入和多输出模型" class="headerlink" title="多输入和多输出模型"></a>多输入和多输出模型</h4><h5 id="functional-API-使操作大量交织在一起的数据流变得容易。"><a href="#functional-API-使操作大量交织在一起的数据流变得容易。" class="headerlink" title="functional API 使操作大量交织在一起的数据流变得容易。"></a>functional API 使操作大量交织在一起的数据流变得容易。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from keras.layres import Input,Embedding,LSTM,Dense</span><br><span class="line">from keras.models import Model</span><br><span class="line"></span><br><span class="line"># Headline input: meant to receive sequences of 100 integersm,between 1 and 10000.</span><br><span class="line"># Note that we can name any layer by passing it a &quot;name&quot; argument.</span><br><span class="line">main_input = Input(shape=(100,),dtype=&apos;int32&apos;,name=&apos;main_input&apos;)</span><br><span class="line"></span><br><span class="line"># This embedding layer will encode the input sequence</span><br><span class="line"># into a sequence of dense 512-dimensional vectors.</span><br><span class="line">x = Embedding(output_dim=512,input_dim=10000,input_length=100)(main_input)</span><br><span class="line"></span><br><span class="line"># A LSTM will transform the vector sequence into a single vector,</span><br><span class="line"># containing information about the entire sequence</span><br><span class="line">lstm_out = LSTM(32)(x)</span><br><span class="line">auxiliary_output = Dense(1,activation=&apos;sigmoid&apos;,name=&apos;aux_output&apos;)(lstm_out)</span><br><span class="line"></span><br><span class="line">auxiliary_input = Input(shape=(5,),name=&apos;aux_input&apos;)</span><br><span class="line">x = keras.layers.concatenate([lstm_out,auxiliary_input])</span><br><span class="line"></span><br><span class="line"># We stack a deep densely-connected network on top</span><br><span class="line">x = Dense(64,activation=&apos;relu&apos;)(x)</span><br><span class="line">x = Dense(64,activation=&apos;relu&apos;)(x)</span><br><span class="line">x = Dense(64,activation=&apos;relu&apos;)(x)</span><br><span class="line"></span><br><span class="line"># And finally we add the main logistic regression layer</span><br><span class="line">main_output = Dense(1,activation=&apos;sigmoid&apos;,name=&apos;main_output&apos;)(x)</span><br></pre></td></tr></table></figure>

<h5 id="这定义了一个具有两个输入和两个输出的模型："><a href="#这定义了一个具有两个输入和两个输出的模型：" class="headerlink" title="这定义了一个具有两个输入和两个输出的模型："></a>这定义了一个具有两个输入和两个输出的模型：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = Model(inputs=[main_input,auxiliary_input],outputs=[main_output,auxiliary_output])</span><br></pre></td></tr></table></figure>

<h5 id="我们编译模型并为辅助损失分配0-2的权重。要为不同的输出指定不同的loss-weights或loss，可以使用列表或字典。这里我们传递一个损失作为loss参数，因此所有输出都将使用相同的损失。"><a href="#我们编译模型并为辅助损失分配0-2的权重。要为不同的输出指定不同的loss-weights或loss，可以使用列表或字典。这里我们传递一个损失作为loss参数，因此所有输出都将使用相同的损失。" class="headerlink" title="我们编译模型并为辅助损失分配0.2的权重。要为不同的输出指定不同的loss_weights或loss，可以使用列表或字典。这里我们传递一个损失作为loss参数，因此所有输出都将使用相同的损失。"></a>我们编译模型并为辅助损失分配0.2的权重。要为不同的输出指定不同的loss_weights或loss，可以使用列表或字典。这里我们传递一个损失作为<code>loss参数</code>，因此所有输出都将使用相同的损失。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=&apos;rmsprop&apos;,loss=&apos;binary_crossentropy&apos;,</span><br><span class="line">              loss_weights=[1.,0.2])</span><br></pre></td></tr></table></figure>

<h5 id="我们可以通过传递输入数组和目标数组的列表来训练模型："><a href="#我们可以通过传递输入数组和目标数组的列表来训练模型：" class="headerlink" title="我们可以通过传递输入数组和目标数组的列表来训练模型："></a>我们可以通过传递输入数组和目标数组的列表来训练模型：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.fit([heading_data,additional_data],[labels,labels],</span><br><span class="line">          epochs=50,batch_size=32)</span><br></pre></td></tr></table></figure>

<h5 id="由于我们的输入和输出被命名（我们传递了一个“name”参数），我们也可以通过以下方式编译模型："><a href="#由于我们的输入和输出被命名（我们传递了一个“name”参数），我们也可以通过以下方式编译模型：" class="headerlink" title="由于我们的输入和输出被命名（我们传递了一个“name”参数），我们也可以通过以下方式编译模型："></a>由于我们的输入和输出被命名（我们传递了一个“<code>name</code>”参数），我们也可以通过以下方式编译模型：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=&apos;rmsprop&apos;,</span><br><span class="line">              loss=&#123;&apos;main_output&apos;:&apos;binary_crossentropy&apos;,&apos;aux_output&apos;:&apos;binary_crossentropy&apos;&#125;,</span><br><span class="line">              loss_weight=&#123;&apos;main_output&apos;:1.,&apos;aux_output&apos;:0.2&#125;)</span><br><span class="line">              </span><br><span class="line"># And trained it via:</span><br><span class="line">model.fit(&#123;&apos;main_input&apos;:headline_data,&apos;aux_output&apos;:additional_data&#125;,</span><br><span class="line">          &#123;&apos;main_output&apos;:labels,&apos;aux_ouput&apos;:labels&#125;,</span><br><span class="line">          epochs=50,batch_size=32)</span><br></pre></td></tr></table></figure>

<h5 id="共享层"><a href="#共享层" class="headerlink" title="共享层"></a>共享层</h5><p>使用函数API的另一个好处是模型可以使用共享层。<br>例子：<br>建立一个模型来判断两条推特是不是同一个用户所发。（可以通过比较推文的相似性来确定）<br>将两条推文编码成两个向量并连接，添加逻辑回归层，这将输出两条推文来自同一用户的概率。使用来自同一用户的两条推文（正），不是同一用户所发的两条推文（负）来训练模型。<br>由于问题是对称的，编码第一条推文的机制（包括权重等）将被重用来编码第二条推文。这里使用LSTM层来编码推文。<br>使用函数式 API 来构建模型。首先我们将一条推特转换为一个尺寸为 (280, 256) 的矩阵，即每条推特 280 字符，每个字符为 256 维的 one-hot 编码向量 （取 256 个常用字符）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import keras</span><br><span class="line">from keras.layers import Input,LSTM,Dense</span><br><span class="line">from keras.models import Model</span><br><span class="line"></span><br><span class="line">tweet_a = Input(shape=(280,256)</span><br><span class="line">tweet_b = Input(shape=(280,256)</span><br></pre></td></tr></table></figure>

<p>要在不同的输入上共享同一个层，只需实例化该层一次，然后根据需要传入你想要的输入即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># This layer can take as input a matrix</span><br><span class="line"># and will return a vector of size 64</span><br><span class="line">shared_lstm = LSTM(64)</span><br><span class="line"></span><br><span class="line"># When we reuse the same layer instance</span><br><span class="line"># multiple times,the weights of layer</span><br><span class="line"># are also being reused</span><br><span class="line"># (it is effectively *the same* layer)</span><br><span class="line">encoded_a = shared_lstm(tweet_a)</span><br><span class="line">encoded_b = shared_lstm(tweet_b)</span><br><span class="line"></span><br><span class="line"># We can then concatenate the two vectors:</span><br><span class="line">merged_vector = keras.layers.concatenate([encoded_a,encoded_b]),axis=-1)</span><br><span class="line"></span><br><span class="line"># And add a logistic regression on top</span><br><span class="line">predictions = Dense(1,activation=&apos;sigmoid&apos;)(merged_vector)</span><br><span class="line"></span><br><span class="line"># We defined a trainable model linking the</span><br><span class="line"># tweet inputs to the predictions</span><br><span class="line"># 定义一个连接推特输入和预测的可训练的模型</span><br><span class="line">model = Model(inputs=[tweet_a,tweet_b],outputs=predictions)</span><br><span class="line">model.compile(optimizer=&apos;rmsprop&apos;,</span><br><span class="line">              loss=&apos;binary_crossentropy&apos;,</span><br><span class="line">              metrics=[&apos;accuracy&apos;])</span><br><span class="line">model.fit([data_a,data_b],labels,epochs=10)</span><br></pre></td></tr></table></figure>

<h5 id="如何读取共享层的输出或输出尺寸"><a href="#如何读取共享层的输出或输出尺寸" class="headerlink" title="如何读取共享层的输出或输出尺寸?"></a>如何读取共享层的输出或输出尺寸?</h5><h4 id="层节点（The-concept-of-layer-“node”）"><a href="#层节点（The-concept-of-layer-“node”）" class="headerlink" title="层节点（The concept of layer “node”）"></a>层节点（The concept of layer “node”）</h4><p>每当你在某个输入上调用一个层时，都将创建一个新的张量（层的输出），并且为该层添加一个「节点」，将输入张量连接到输出张量。当多次调用同一个图层时，该图层将拥有多个节点索引 (0, 1, 2…)。<br>在之前版本的 Keras 中，可以通过 <code>layer.get_output()</code> 来获得层实例的输出张量，或者通过 <code>layer.output_shape</code> 来获取其输出形状。现在你依然可以这么做（除了 <code>get_output()</code> 已经被 <code>output</code> 属性替代）。但是如果一个层与多个输入连接呢？</p>
<p>只要一个层仅仅连接到一个输入，就不会有困惑，<code>.output</code> 会返回层的唯一输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = Input(shape=(280,256))</span><br><span class="line">lstm = LSTM(32)</span><br><span class="line">encoded_a = lstm(a)</span><br><span class="line">assert lstm.output == encoded_a</span><br></pre></td></tr></table></figure>

<p>但是如果该层有多个输入，那就会出现问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = Input(shape=(280,256))</span><br><span class="line">b = Input(shape=(280,256))</span><br><span class="line"></span><br><span class="line">lstm = LSTM(32)</span><br><span class="line">encoded_a = lstm(a)</span><br><span class="line">encoded_b = lstm(b)</span><br><span class="line"></span><br><span class="line">lstm.output</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; AttributeError: Layer lstm_1 has multiple inbound nodes,</span><br><span class="line">hence the notion of &quot;layer output&quot; is ill-defined.</span><br><span class="line">Use `get_output_at(node_index)` instead.</span><br></pre></td></tr></table></figure>

<p>解决方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">assert lstm.get_output_at(0) == encoded_a</span><br><span class="line">assert lstm.get_output_at(1) == encoded_b</span><br></pre></td></tr></table></figure>

<p><code>input_shape</code> 和 <code>output_shape</code> 这两个属性也是如此：只要该层只有一个节点，或者只要所有节点具有相同的输入/输出尺寸，那么「层输出/输入尺寸」的概念就被很好地定义，并且将由 <code>layer.output_shape / layer.input_shape</code> 返回。但是比如说，如果将一个 <code>Conv2D</code> 层先应用于尺寸为 <code>(32，32，3)</code> 的输入，再应用于尺寸为 <code>(64, 64, 3)</code> 的输入，那么这个层就会有多个输入/输出尺寸，你将不得不通过指定它们所属节点的索引来获取它们：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = Input(shape=(32,32,3))</span><br><span class="line">b = Input(shape=(64,64,3))</span><br><span class="line"></span><br><span class="line">conv = Conv2D(16,(3,3),padding=&apos;same&apos;)</span><br><span class="line">conved_a = conv(a)</span><br><span class="line"></span><br><span class="line">#Only one input so far,the following will work:</span><br><span class="line">assert conv.input_shape == (None,32,32,3)</span><br><span class="line"></span><br><span class="line">conved_b = conv(b)</span><br><span class="line"># now the &apos;.input_shape&apos; property wouldn&apos;t work,but this does:</span><br><span class="line">assert conv.get_input_shape_at(0) == (None,32,32,3)</span><br><span class="line">assert conv.get_input_shape_at(1) == (None,64,64,3)</span><br></pre></td></tr></table></figure>

<h4 id="更多的例子"><a href="#更多的例子" class="headerlink" title="更多的例子"></a>更多的例子</h4><h5 id="Inception模型"><a href="#Inception模型" class="headerlink" title="Inception模型"></a>Inception模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Conv2D,MaxPooling2D,Input</span><br><span class="line"></span><br><span class="line">input_img = Input(shape=(256,256,3))</span><br><span class="line"></span><br><span class="line">tower_1 = Conv2D(64,(1,1),padding=&apos;same&apos;,activation=&apos;relu&apos;)(input_img)</span><br><span class="line">tower_1 = Conv2D(64,(3,3),padding=&apos;same&apos;,activation=&apos;relu&apos;)(tower_1)</span><br><span class="line"></span><br><span class="line">tower_2 = Conv2D(64,(1,1),padding=&apos;same&apos;,activation=&apos;relu&apos;)(input_img)</span><br><span class="line">tower_2 = Conv2D(64,(5,5),padding=&apos;same&apos;,activation=&apos;relu&apos;)(tower_2)</span><br><span class="line"></span><br><span class="line">tower_3 = MaxPooling2D((3,3),strides=(1,1),padding=&apos;same&apos;)(input_img)</span><br><span class="line">tower_3 = Conv2D(64,(1,1),padding=&apos;same&apos;,activation=&apos;relu&apos;)(tower_3)</span><br><span class="line"></span><br><span class="line">output = keras.layers.concatenate([tower_1,tower_2,tower_3],axis=1)</span><br></pre></td></tr></table></figure>

<h5 id="卷积层上的残差连接"><a href="#卷积层上的残差连接" class="headerlink" title="卷积层上的残差连接"></a>卷积层上的残差连接</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Conv2D,Input</span><br><span class="line"># input tensor for a 3-channel 256x256 image</span><br><span class="line">x = Input(shape=(256,256,3))</span><br><span class="line"># 3x3 conv with 3 output channels (same as input channels)</span><br><span class="line">y = Conv2D(3,(3,3),padding=&apos;same&apos;)(x)</span><br><span class="line"># this returns x+y</span><br><span class="line">z = keras.layers.add([x,y])</span><br></pre></td></tr></table></figure>

<h5 id="共享视觉模型"><a href="#共享视觉模型" class="headerlink" title="共享视觉模型"></a>共享视觉模型</h5><p>该模型在两个输入上重复使用同一个图像处理模块，以判断两个 MNIST 数字是否为相同的数字。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Conv2D,MaxPooling2D,Input,Dense,Flatten</span><br><span class="line">from keras.models import Model</span><br><span class="line"></span><br><span class="line"># First,define the vision modules</span><br><span class="line">digit_input = Input(shape=(27,27,1))</span><br><span class="line">x = Conv2D(64,(3,3))(digit_input)</span><br><span class="line">x = Conv2D(64,(3,3))(x)</span><br><span class="line">x = MaxPooling2D((2,2))(x)</span><br><span class="line">out = Flatten()(x)</span><br><span class="line"></span><br><span class="line">vision_model = Model(digit_input,out)</span><br><span class="line"></span><br><span class="line"># Then define the tell-digits-apart model</span><br><span class="line">digit_a = Input(shape=(27,27,1))</span><br><span class="line">digit_b = Input(shape=(27,27,1))</span><br><span class="line"></span><br><span class="line"># The vision model will be shared,weights and all</span><br><span class="line">out_a = vision_model(digit_a)</span><br><span class="line">out_b = vision_model(digit_b)</span><br><span class="line"></span><br><span class="line">concatenated = keras.layers.concatenate([out_a,out_b])</span><br><span class="line">out = Dense(1,activation=&apos;sigmoid&apos;)(concatenated)</span><br><span class="line"></span><br><span class="line">classification_model = Model([digit_a,digit_b],out)</span><br></pre></td></tr></table></figure>

<h5 id="视觉问答模型"><a href="#视觉问答模型" class="headerlink" title="视觉问答模型"></a>视觉问答模型</h5><p>当被问及关于图片的自然语言问题时，该模型可以选择正确的单词作答。<br>它通过将问题和图像编码成向量，然后连接两者，在上面训练一个逻辑回归，来从词汇表中挑选一个可能的单词作答。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import Conv2D,MaxPooling2D,Flatten</span><br><span class="line">from keras.layers import Input,LSTM,Embedding,Dense</span><br><span class="line">from keras.models import Model,Sequential</span><br><span class="line"># First,let&apos;s define a vision model using a Sequential model</span><br><span class="line"># This model will encode an image into a vector</span><br><span class="line">vision_model = Sequential()</span><br><span class="line">vision_model.add(Conv2D(64,(3,3),activation=&apos;relu&apos;,padding=&apos;same&apos;,input_shape=(224,224,3)))</span><br><span class="line">vision_model.add(Conv2D(64,(3,3),activation=&apos;relu&apos;))</span><br><span class="line">vision_model.add(MaxPooling2D((2,2)))</span><br><span class="line">vision_model.add(Conv2D(128,(3,3),activation=&apos;relu&apos;,padding=&apos;same&apos;))</span><br><span class="line">vision_model.add(Conv2D(128,(3,3),activation=&apos;relu&apos;))</span><br><span class="line">vision_model.add(MaxPooling2D((2,2)))</span><br><span class="line">vision_model.add(Conv2D(256,(3,3),activation=&apos;relu&apos;,padding=&apos;same&apos;))</span><br><span class="line">vision_model.add(Conv2D(256,(3,3),activation=&apos;relu&apos;))</span><br><span class="line">vision_model.add(Conv2D(256,(3,3),activation=&apos;relu&apos;))</span><br><span class="line">vision_model.add(MaxPooling2D((2,2)))</span><br><span class="line">vision_model.add(Flatten())</span><br><span class="line"></span><br><span class="line"># Now let&apos;s get a tensor with the output of our vision model:</span><br><span class="line">image_input = Input(shape=(224,224,3))</span><br><span class="line">encoded_image = vision_model(image_input)</span><br><span class="line"></span><br><span class="line"># Next,let&apos;s define a language model to encode the question into a vector.</span><br><span class="line"># Each question will be at most 100 word long,</span><br><span class="line"># and we will index words as integers from 1 to 9999</span><br><span class="line">question_input = Input(shape=(100,),dtype=&apos;int32&apos;)</span><br><span class="line">embedded_question = Embedding(input_dim=10000,output_dim=256,input_length=100)(question_input)</span><br><span class="line">encoded_question = LSTM(256)(embedded_question)</span><br><span class="line"></span><br><span class="line"># Let&apos;s concatenate the question vector and the image vector:</span><br><span class="line">merged = keras.layers.concatenate([encoded_question,encoded_image])</span><br><span class="line"></span><br><span class="line"># And let&apos;s train a logistic regression over 1000 words on top:</span><br><span class="line">output = Dense(1000,activation=&apos;softmax&apos;)(merged)</span><br><span class="line"></span><br><span class="line"># This is our final model:</span><br><span class="line">vqa_model = Model(inputs=[image_input,question_input],outputs=output)</span><br><span class="line"># The next stage would be training this model on actual data.</span><br></pre></td></tr></table></figure>

<h5 id="视频问答模型"><a href="#视频问答模型" class="headerlink" title="视频问答模型"></a>视频问答模型</h5><p>现在我们已经训练了图像问答模型，我们可以很快地将它转换为视频问答模型。在适当的训练下，你可以给它展示一小段视频（例如 100 帧的人体动作），然后问它一个关于这段视频的问题（例如，「这个人在做什么运动？」 -&gt; 「足球」）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from keras.layers import TimeDistributed</span><br><span class="line"></span><br><span class="line">video_input = Input(shape=(100, 224, 224, 3))</span><br><span class="line"># 这是基于之前定义的视觉模型（权重被重用）构建的视频编码</span><br><span class="line">encoded_frame_sequence = TimeDistributed(vision_model)(video_input)  # 输出为向量的序列</span><br><span class="line">encoded_video = LSTM(256)(encoded_frame_sequence)  # 输出为一个向量</span><br><span class="line"></span><br><span class="line"># 这是问题编码器的模型级表示，重复使用与之前相同的权重：</span><br><span class="line">question_encoder = Model(inputs=question_input, outputs=encoded_question)</span><br><span class="line"></span><br><span class="line"># 让我们用它来编码这个问题：</span><br><span class="line">video_question_input = Input(shape=(100,), dtype=&apos;int32&apos;)</span><br><span class="line">encoded_video_question = question_encoder(video_question_input)</span><br><span class="line"></span><br><span class="line"># 这就是我们的视频问答模式：</span><br><span class="line">merged = keras.layers.concatenate([encoded_video, encoded_video_question])</span><br><span class="line">output = Dense(1000, activation=&apos;softmax&apos;)(merged)</span><br><span class="line">video_qa_model = Model(inputs=[video_input, video_question_input], outputs=output)</span><br></pre></td></tr></table></figure>


      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/08/14/meshgrid/" rel="next" title="np.meshgrid">
                <i class="fa fa-chevron-left"></i> np.meshgrid
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/08/14/Python数据结构第一章/" rel="prev" title>
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Zhaohui Bai</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/BaiZhaohui" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:isolatedislet@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Keras-functional-API"><span class="nav-number">1.</span> <span class="nav-text">Keras functional API</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Keras-functional-API-是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。"><span class="nav-number">1.0.1.</span> <span class="nav-text">Keras functional API 是一种用来定义复杂模型（如多输出模型、有向无环图或具有共享层的模型）的方法。</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#例1：全连接网络"><span class="nav-number">1.1.</span> <span class="nav-text">例1：全连接网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#所有模型都可以像层一样调用"><span class="nav-number">1.2.</span> <span class="nav-text">所有模型都可以像层一样调用</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#使用功能API，可以轻松地重用经过训练的模型：您可以通过在张量上调用任何模型来将其视为一个层。请注意，通过调用模型，您不仅可以重用模型的体系结构，还可以重用其权重。"><span class="nav-number">1.2.1.</span> <span class="nav-text">使用功能API，可以轻松地重用经过训练的模型：您可以通过在张量上调用任何模型来将其视为一个层。请注意，通过调用模型，您不仅可以重用模型的体系结构，还可以重用其权重。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#这可以允许快速创建可以处理输入序列的模型。您可以将图像分类模型转换为视频分类模型，只需一行。"><span class="nav-number">1.2.2.</span> <span class="nav-text">这可以允许快速创建可以处理输入序列的模型。您可以将图像分类模型转换为视频分类模型，只需一行。</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多输入和多输出模型"><span class="nav-number">1.3.</span> <span class="nav-text">多输入和多输出模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#functional-API-使操作大量交织在一起的数据流变得容易。"><span class="nav-number">1.3.1.</span> <span class="nav-text">functional API 使操作大量交织在一起的数据流变得容易。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#这定义了一个具有两个输入和两个输出的模型："><span class="nav-number">1.3.2.</span> <span class="nav-text">这定义了一个具有两个输入和两个输出的模型：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#我们编译模型并为辅助损失分配0-2的权重。要为不同的输出指定不同的loss-weights或loss，可以使用列表或字典。这里我们传递一个损失作为loss参数，因此所有输出都将使用相同的损失。"><span class="nav-number">1.3.3.</span> <span class="nav-text">我们编译模型并为辅助损失分配0.2的权重。要为不同的输出指定不同的loss_weights或loss，可以使用列表或字典。这里我们传递一个损失作为loss参数，因此所有输出都将使用相同的损失。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#我们可以通过传递输入数组和目标数组的列表来训练模型："><span class="nav-number">1.3.4.</span> <span class="nav-text">我们可以通过传递输入数组和目标数组的列表来训练模型：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#由于我们的输入和输出被命名（我们传递了一个“name”参数），我们也可以通过以下方式编译模型："><span class="nav-number">1.3.5.</span> <span class="nav-text">由于我们的输入和输出被命名（我们传递了一个“name”参数），我们也可以通过以下方式编译模型：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#共享层"><span class="nav-number">1.3.6.</span> <span class="nav-text">共享层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#如何读取共享层的输出或输出尺寸"><span class="nav-number">1.3.7.</span> <span class="nav-text">如何读取共享层的输出或输出尺寸?</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#层节点（The-concept-of-layer-“node”）"><span class="nav-number">1.4.</span> <span class="nav-text">层节点（The concept of layer “node”）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#更多的例子"><span class="nav-number">1.5.</span> <span class="nav-text">更多的例子</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Inception模型"><span class="nav-number">1.5.1.</span> <span class="nav-text">Inception模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#卷积层上的残差连接"><span class="nav-number">1.5.2.</span> <span class="nav-text">卷积层上的残差连接</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#共享视觉模型"><span class="nav-number">1.5.3.</span> <span class="nav-text">共享视觉模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#视觉问答模型"><span class="nav-number">1.5.4.</span> <span class="nav-text">视觉问答模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#视频问答模型"><span class="nav-number">1.5.5.</span> <span class="nav-text">视频问答模型</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhaohui Bai</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>








        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
